{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7502d5b",
   "metadata": {},
   "source": [
    "# P5: Deep RL, Bandits & MCTS\n",
    "\n",
    "[Instructions are the same as they have been. You know the drill by now.]\n",
    "\n",
    "\n",
    "## P5.1 Deep Reinforcement Learning [TODO]\n",
    "\n",
    "In the last programming assignment, you were asked to implement Q learning. Since then, we've discussed *Deep Reinforcement Learning*, in which deep learning is used to learn to approximate the Q function. While deep learning is somewhat out of scope for this class, I thought it would be instructive for you to follow a tutorial for Deep RL and tune the parameters a bit. The following link takes you to a Google Collab notebook, which lets you connect to a (free, with a Google account) cloud GPU on which the experiments will be run:\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\n",
    "\n",
    "We will be experimenting with the 'cartpole' environment, in which an agent seeks to move a cart so as to balance a pole vertically upwards. The agent is rewarded for each time step in which the pole is vertical. This is a common environment for debugging and evaluating reinforcement learning algorithms.\n",
    "\n",
    "**QUESTION** For this scenario, what is the maximum total reward for the Cartpole problem (this is written out in the tutorial notebook)?\n",
    "\n",
    "**TASK** Run all the tutorial code without modification. [Navigate to \"Runtime\" in the menu bar and then \"Run all\".]\n",
    "\n",
    "**RESULT** Before the machine learning algorithm is trained, the function `compute_avg_return` is used to evaluate the performance of a random policy. Include in your report the value of that random policy.\n",
    "\n",
    "**PLOT** After the code is complete, it will generate a plot of \"Average Return vs. Iterations\". Include this plot in your writeup.\n",
    "\n",
    "**QUESTION** How does the algorithm perform? How does the final average return compare to the maximum possible value?\n",
    "\n",
    "**TASK/PLOT** Under the \"Hyperparameters\" section, there are a few parameters you can tune. Next, we will change the `learning_rate` (initially set to `1e-3`), which controls the rate at which the parameters of the neural network that estimates Q are tuned. Rerun all the code (via \"Run all\") for `learning_rate` set to `1e-4`). Include the *clearly labeled* plot of Average Return vs. Iterations.\n",
    "\n",
    "**QUESTION** (3-6 sentences) Describe how the learning rate changes performance. Does the final average return change? Does the rate of improvement change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddf95f",
   "metadata": {},
   "source": [
    "## P5.2 Bandit Algorithms\n",
    "\n",
    "Here, you'll be looking in more depth at the Bandit Algorithms we discussed in class. To begin, you are given the same example we saw in the in-class breakout session. You are provided with 10 bandits of an unknown (to the agent) reward distribution. The plot generated in the first plot shows the rewards generated by \"pulling\" the arms of each bandit.\n",
    "\n",
    "### P5.2.1 Epsilon Greedy Bandits\n",
    "\n",
    "I have given you starter code for the epsilon-greedy bandit algorithm we discussed in class.\n",
    "\n",
    "**TASK** Complete `bandit_epsilon_greedy` and run the evaluation code that follows.\n",
    "\n",
    "**PLOTS** Include the set of plots (in a single figure) generated by the evaluation code.\n",
    "\n",
    "**RESULTS** Include in your report the (average) percentage of optimal pulls for each value of epsilon.\n",
    "\n",
    "**QUESTION** (2-4 sentences) For which epsilon is the peak \"optimal pull percentage\"? Why does the percentage of optimal pulls (and, relatedly the average reward) decrease for the higher values of epsilon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-armed bandit\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed(695)\n",
    "\n",
    "# Generate some bandits\n",
    "class Bandit(object):\n",
    "    def __init__(self, mean=None, mean_range=[-1, 3], sigma=1.0):\n",
    "        if mean is None:\n",
    "            self.mean = random.uniform(min(mean_range), max(mean_range))\n",
    "        else:\n",
    "            self.mean = mean\n",
    "        self.mean_range = mean_range\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def pull_arm(self):\n",
    "        return random.gauss(self.mean, self.sigma)\n",
    "    \n",
    "    def get_data_for_plot(self, width=0.05, num_values=2000):\n",
    "        xs = np.random.normal(0, width, num_values)\n",
    "        ys = np.random.normal(self.mean, self.sigma, num_values)\n",
    "        return xs, ys\n",
    "\n",
    "def plot_bandits(bandits):\n",
    "    for ii, bandit in enumerate(bandits):\n",
    "        xs, ys = bandit.get_data_for_plot()\n",
    "        plt.scatter(xs + ii, ys, alpha=0.01)\n",
    "        plt.plot(ii, bandit.mean, 'ko', alpha=0.5)\n",
    "    \n",
    "num_bandits = 10\n",
    "bandits = [Bandit() for _ in range(8)]\n",
    "bandits.append(Bandit(mean=3.0))\n",
    "bandits.append(Bandit(mean=3.1))\n",
    "random.shuffle(bandits)\n",
    "plot_bandits(bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bandit starter code\n",
    "\n",
    "def bandit_epsilon_greedy(bandits, num_steps=2000, epsilon=0.0, seed=695):\n",
    "    \"\"\"\n",
    "    returns (average reward over time,\n",
    "             proportion of pulls for each bandit by end)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    num_pulls_per_bandit = np.zeros(len(bandits))\n",
    "    tot_reward_per_bandit = np.zeros(len(bandits))\n",
    "    all_rewards = []  # List of each reward returned per pull.\n",
    "    \n",
    "    for ii in range(num_steps):\n",
    "        # Pick one of the bandits.\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Data Storage\n",
    "        reward = bandits[bandit_ind].pull_arm()\n",
    "        tot_reward_per_bandit[bandit_ind] += reward\n",
    "        num_pulls_per_bandit[bandit_ind] += 1\n",
    "        all_rewards.append(reward)\n",
    "    \n",
    "    return (np.cumsum(all_rewards) / (np.arange(num_steps) + 1),\n",
    "            num_pulls_per_bandit / num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Code\n",
    "plt.figure(figsize=(12, 4))\n",
    "epsilons = [0.001, 0.05, 0.1, 0.2, 0.5]\n",
    "best_bandit_ind = bandits.index(max(bandits, key=lambda b: b.mean))\n",
    "for ii, epsilon in enumerate(epsilons):\n",
    "    dat = [bandit_epsilon_greedy(bandits, epsilon=epsilon, seed=seed)\n",
    "                for seed in range(100)]\n",
    "    all_runs, pull_rates = zip(*dat)\n",
    "    plt.subplot(1, len(epsilons), ii+1)\n",
    "    avg_rewards = np.mean(all_runs, axis=0)\n",
    "    p10_rewards = np.percentile(all_runs, 10, axis=0)\n",
    "    p30_rewards = np.percentile(all_runs, 30, axis=0)\n",
    "    p70_rewards = np.percentile(all_runs, 70, axis=0)\n",
    "    p90_rewards = np.percentile(all_runs, 90, axis=0)\n",
    "    plt.plot(avg_rewards)\n",
    "    plt.fill_between(range(len(avg_rewards)),\n",
    "                     p30_rewards,\n",
    "                     p70_rewards,\n",
    "                     alpha=0.1, color='blue')\n",
    "    plt.fill_between(range(len(avg_rewards)),\n",
    "                     p10_rewards,\n",
    "                     p90_rewards,\n",
    "                     alpha=0.1, color='blue')\n",
    "    print(f\"Eps = {epsilon:0.3f} : Optimal Pulls = \"\n",
    "          f\"{100*np.mean(pull_rates, axis=0)[best_bandit_ind]:6.2f}%\")\n",
    "    plt.title(f\"Eps: {epsilon:0.3f}\")\n",
    "    plt.ylim([0, 3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16505ddb",
   "metadata": {},
   "source": [
    "### P5.2.2 UCB Bandits\n",
    "\n",
    "**TASK** Implement the UCB Bandit function `bandit_ucb(bandits, num_steps=2000, c=4.0, seed=695)` and run the evaluation code that follows. Your `bandit_ucb` implementation will look a lot like the `bandit_epsilon_greedy`, so you may use that as starter code.\n",
    "\n",
    "**CODE** Include your implementation of `bandit_ucb` in your writeup.\n",
    "\n",
    "**PLOTS** Include the set of plots (in a single figure) generated by the evaluation code.\n",
    "\n",
    "**RESULTS** Include in your report the (average) percentage of optimal pulls for each value of C (the exploration parameter).\n",
    "\n",
    "**QUESTION** (2-5 sentences) For which C is the peak \"optimal pull percentage\"? How does the percentage of optimal pulls for this value of C compare to the best epsilon from the epsilon-greedy bandit? Why?\n",
    "\n",
    "**QUESTION** (2-5 sentences) Why is the average performance of the algorithm poor for very low values of C? What will happen to the performance as the number of trials approach infinity?\n",
    "\n",
    "**QUESTION** (2-5 sentences) Why is the average performance of the algorithm poor for very high values of C? What will happen to the performance as the number of trials approach infinity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e224d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Code\n",
    "plt.figure(figsize=(12, 4))\n",
    "cs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "best_bandit_ind = bandits.index(max(bandits, key=lambda b: b.mean))\n",
    "for ii, c in enumerate(cs):\n",
    "    dat = [bandit_ucb(bandits, c=c, seed=seed)\n",
    "           for seed in range(100)]\n",
    "    all_runs, pull_rates = zip(*dat)\n",
    "    plt.subplot(1, len(cs), ii+1)\n",
    "    avg_rewards = np.mean(all_runs, axis=0)\n",
    "    p10_rewards = np.percentile(all_runs, 10, axis=0)\n",
    "    p30_rewards = np.percentile(all_runs, 30, axis=0)\n",
    "    p70_rewards = np.percentile(all_runs, 70, axis=0)\n",
    "    p90_rewards = np.percentile(all_runs, 90, axis=0)\n",
    "    plt.plot(avg_rewards)\n",
    "    plt.fill_between(range(len(avg_rewards)),\n",
    "                     p30_rewards,\n",
    "                     p70_rewards,\n",
    "                     alpha=0.1, color='blue')\n",
    "    plt.fill_between(range(len(avg_rewards)),\n",
    "                     p10_rewards,\n",
    "                     p90_rewards,\n",
    "                     alpha=0.1, color='blue')\n",
    "    print(f\"c = {c:0.5f} : Optimal Pulls = \"\n",
    "          f\"{100*np.mean(pull_rates, axis=0)[best_bandit_ind]:6.2f}%\")\n",
    "    plt.title(f\"c: {c:0.5f}\")\n",
    "    plt.ylim([0, 3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e357ed",
   "metadata": {},
   "source": [
    "## P5.3 Connect Four and MCTS\n",
    "\n",
    "I have provided you with a (pure python) implementation of the popular turn-based game \"Connect Four\". In this game, you 'drop' pieces of different color into the game board, trying to get four in a row (along any axis) before your opponent can. In this question, I have provided you with an implementation of minimax search, which you will use to compare against your own implementation of Monte Carlo Tree Search.\n",
    "\n",
    "### P5.3.1 Minimax\n",
    "\n",
    "I have provided you with an already-complete minimax algorithm implementation. The algorithm has an element of randomness in it: if it multiple actions are of equal value, it picks one at random. The evaluation code below pits two different minimax algorithms against one another: one that runs at depth 5 and one that runs at depth 3.\n",
    "\n",
    "**QUESTION** (2-3 sentences) What is the evaluation function being used to evaluate the goodness of a board state once the maximum depth is reached? How \"useful\" is the value function I have provided?\n",
    "\n",
    "**TASK** Run the evaluation code below and observe the results.\n",
    "\n",
    "**RESULTS** Include the win/draw counts in your writeup.\n",
    "\n",
    "**QUESTION** (1-3 sentences) You may notice that sometimes the depth-3 minimax search wins against the depth-5 minimax search. How is this possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect Four Implementation\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class ConnectFourBoard(object):\n",
    "    def __init__(self, nrows=6, ncols=7):\n",
    "        self.current_player = 1\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "        self.board = np.zeros((nrows, ncols), int)\n",
    "        self.winner = None\n",
    "\n",
    "    def get_moves(self):\n",
    "        return np.where(self.board[0] == 0)[0]\n",
    "\n",
    "    def _check_winner(self, row, col):\n",
    "        \"\"\"Check if the move in col ended the game.\n",
    "        Not wonderful code, but easy to follow and debug.\"\"\"\n",
    "        b, r, c = self.board, row, col\n",
    "        \n",
    "        # Diagonal 1\n",
    "        if r - 3 >= 0 and c - 3 >= 0:\n",
    "            if b[r, c] == b[r-1, c-1] == b[r-2, c-2] == b[r-3, c-3]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r - 2 >= 0 and c - 2 >= 0 and r + 1 < self.nrows and c + 1 < self.ncols:\n",
    "            if b[r+1, c+1] == b[r, c] == b[r-1, c-1] == b[r-2, c-2]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r - 1 >= 0 and c - 1 >= 0 and r + 2 < self.nrows and c + 2 < self.ncols:\n",
    "            if b[r+2, c+2] == b[r+1, c+1] == b[r, c] == b[r-1, c-1]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r + 3 < self.nrows and c + 3 < self.ncols:\n",
    "            if b[r+3, c+3] == b[r+2, c+2] == b[r+1, c+1] == b[r, c]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "                \n",
    "        # Diagonal 2\n",
    "        if r - 3 >= 0 and c + 3 < self.ncols:\n",
    "            if b[r, c] == b[r-1, c+1] == b[r-2, c+2] == b[r-3, c+3]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r - 2 >= 0 and c + 2 < self.ncols and r + 1 < self.nrows and c - 1 >= 0:\n",
    "            if b[r+1, c-1] == b[r, c] == b[r-1, c+1] == b[r-2, c+2]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r - 1 >= 0 and c + 1 < self.ncols and r + 2 < self.nrows and c - 2 >= 0:\n",
    "            if b[r+2, c-2] == b[r+1, c-1] == b[r, c] == b[r-1, c+1]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r + 3 < self.nrows and c - 3 >= 0:\n",
    "            if b[r+3, c-3] == b[r+2, c-2] == b[r+1, c-1] == b[r, c]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "                \n",
    "        # Flat\n",
    "        if c + 3 < self.ncols:\n",
    "            if b[r, c] == b[r, c+1] == b[r, c+2] == b[r, c+3]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if c + 2 < self.ncols and c - 1 >= 0:\n",
    "            if b[r, c-1] == b[r, c] == b[r, c+1] == b[r, c+2]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if c + 1 < self.ncols and c - 2 >= 0:\n",
    "            if b[r, c-2] == b[r, c-1] == b[r, c] == b[r, c+1]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if c - 3 >= 0:\n",
    "            if b[r, c-3] == b[r, c-2] == b[r, c-1] == b[r, c]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "                \n",
    "        # Down\n",
    "        if r - 3 >= 0:\n",
    "            if b[r, c] == b[r-1, c] == b[r-2, c] == b[r-3, c]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "        if r + 3 < self.nrows:\n",
    "            if b[r+3, c] == b[r+2, c] == b[r+1, c] == b[r, c]:\n",
    "                self.winner = b[r, c]\n",
    "                return\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def play_move(self, col):\n",
    "        try:\n",
    "            row = np.where(self.board[:, col] == 0)[0][-1]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"Cannot play column '{col}'.\")\n",
    "        self.board[row, col] = self.current_player\n",
    "\n",
    "        # Check for a winner\n",
    "        self._check_winner(row, col)\n",
    "        \n",
    "        # Check if no more moves\n",
    "        if len(self.get_moves()) == 0:\n",
    "            self.winner = 0\n",
    "        \n",
    "        # Switch player\n",
    "        if self.current_player == 1:\n",
    "            self.current_player = 2\n",
    "        else:\n",
    "            self.current_player = 1\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def play_random_move(self):\n",
    "        self.play_move(random.choice(self.get_moves()))\n",
    "    \n",
    "    def play_random_moves_until_done(self):\n",
    "        while self.winner is None:\n",
    "            self.play_random_move()\n",
    "\n",
    "        return self.winner\n",
    "            \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        print()\n",
    "        for row in self.board:\n",
    "            string += f\"|{row}|\\n\"\n",
    "        string += '=' * (2 * self.ncols + 3)\n",
    "        \n",
    "        return string\n",
    "    \n",
    "# Some simple tests\n",
    "print(\"A board and randomly playing moves.\")\n",
    "board = ConnectFourBoard(nrows=6, ncols=7)\n",
    "print(board)\n",
    "print(board.get_moves())\n",
    "\n",
    "while board.winner is None:\n",
    "    board.play_random_move()\n",
    "\n",
    "print(board)\n",
    "print(board.winner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimax search\n",
    "def minimax(state, depth, is_max_turn=True, is_start=True, player=None, verbose=False):\n",
    "    if player is None:\n",
    "        player = state.current_player\n",
    "    if depth == 0 or state.winner is not None:\n",
    "        if state.winner is None or state.winner == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2 * (state.winner == player) - 1\n",
    "\n",
    "    moves = state.get_moves()\n",
    "    values = [minimax(state.copy().play_move(move), depth - 1, not is_max_turn, \n",
    "                      is_start=False, player=player)\n",
    "              for move in moves]\n",
    "\n",
    "    if is_start:\n",
    "        best_actions = np.where(np.array(values) == max(values))[0]\n",
    "        action_ind = random.choice(best_actions)\n",
    "        if verbose:\n",
    "            print(list(zip(values, moves)), action_ind)\n",
    "        return moves[action_ind]\n",
    "    if is_max_turn:\n",
    "        return max(values)\n",
    "    else:\n",
    "        return min(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f6302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# minimax evaluation code\n",
    "\n",
    "def print_wins_minimax(wins):\n",
    "    print(\"======\")\n",
    "    print(f\"Total Plays: {len(wins)}\")\n",
    "    print(f\"Depth 6 Wins: {len([w for w in wins if w == 1])}\")\n",
    "    print(f\"Depth 4 Wins: {len([w for w in wins if w == 2])}\")\n",
    "    print(f\"Draws: {len([w for w in wins if w == 0])}\")\n",
    "\n",
    "\n",
    "wins = []\n",
    "for _ in range(25):\n",
    "    board = ConnectFourBoard(nrows=6, ncols=7)\n",
    "    board.current_player = random.choice([1, 2])\n",
    "    while board.winner is None:\n",
    "        if board.current_player == 1:\n",
    "            action = minimax(board, depth=5)\n",
    "        else:\n",
    "            action = minimax(board, depth=3)\n",
    "        board.play_move(action)\n",
    "    \n",
    "    print(board)\n",
    "    print(board.winner)\n",
    "    wins.append(board.winner)\n",
    "    print_wins_minimax(wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07199f",
   "metadata": {},
   "source": [
    "### P5.3.2 Monte-Carlo Tree Search [8 points]\n",
    "\n",
    "*Note: regardless of which algorithm goes first, MCTS is always represented by the number '2' in the printed out board states and minimax is always '1'*\n",
    "\n",
    "**TASK** Complete the functions `monte_carlo_tree_search`, `backpropagate`, `best_child`, and `best_uct`.\n",
    "\n",
    "**CODE** Include your implementations of `monte_carlo_tree_search`, `backpropagate`, `best_child`, and `best_uct` in your writeup.\n",
    "\n",
    "**TASK** Run the evaluation code below. (Note that it may take a few minutes to run all 25 games.)\n",
    "\n",
    "**QUESTION+RESULTS+PLOTS** For the given configuration (1000 iterations, C=5), which algorithm wins more often? Pick a couple final board states (printed out when one strategy wins) and include them in your writeup (screenshots are acceptable).\n",
    "\n",
    "**QUESTION+RESULTS** Rerun the experiments with C=0.1 and C=25. Include the win rates; how well does MCTS perform when you change C?\n",
    "\n",
    "**QUESTION+PLOTS** (4-5 sentences) Describe how the behavior of the MCTS changes when you change the value of C. Pick a couple final board states (printed out when one strategy wins) that support your conclusion and include them in your writeup (screenshots are acceptable). In your answer, you might consider discussing the types of ways MCTS wins/loses for different values of C. Be sure to label which value of C was used for each final board state you include in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fd00a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Starter Code: MCTS Implementation\n",
    "import time\n",
    "\n",
    "\n",
    "class Tree():\n",
    "    def __init__(self, *, start_state=None, parent=None, move=None):\n",
    "        if parent is None:\n",
    "            self.parent = None\n",
    "            self.move = None\n",
    "            self.state = start_state\n",
    "        else:\n",
    "            self.parent = parent\n",
    "            self.move = move\n",
    "            self.state = parent.state.copy()\n",
    "            self.state.play_move(move)\n",
    "        \n",
    "        self.values = []\n",
    "        self.n = 0\n",
    "        if self.is_terminal_state:\n",
    "            self.unexplored_moves = set()\n",
    "        else:\n",
    "            self.unexplored_moves = set(self.state.get_moves())\n",
    "        self.children = set([])\n",
    "            \n",
    "    @property\n",
    "    def fully_expanded(self):\n",
    "        return len(self.unexplored_moves) == 0\n",
    "    \n",
    "    @property\n",
    "    def is_terminal_state(self):\n",
    "        return (self.state.winner is not None)\n",
    "\n",
    "\n",
    "def monte_carlo_tree_search(start_state, num_iterations=1000):\n",
    "    \"\"\"MCTS core loop\"\"\"\n",
    "    # Start by creating the root of the tree.\n",
    "    root = Tree(start_state=start_state)\n",
    "    \n",
    "    # Loop through MCTS iterations.\n",
    "    for _ in range(num_iterations):\n",
    "        # One step of MCTS iteration\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # When done iterating, return the 'best' child of the root node.\n",
    "    return best_child(root)\n",
    "\n",
    "def best_child(node):\n",
    "    \"\"\"When done sampling, pick the child visited the most.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def best_uct(node, C=5):\n",
    "    \"\"\"Pick the best action according to the UCB/UCT algorithm\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def traverse(node):\n",
    "    # If fully explored, pick one of the children\n",
    "    while node.fully_expanded and not node.is_terminal_state:\n",
    "        node = best_uct(node)\n",
    "    # If the node is terminal, return it\n",
    "    if node.is_terminal_state:\n",
    "        return node\n",
    "    \n",
    "    # If the node is not terminal:\n",
    "    # 1. pick a new move from 'unexplored_moves'\n",
    "    move = node.unexplored_moves.pop()\n",
    "    # 2. create a new child\n",
    "    new_child = Tree(parent=node, move=move)\n",
    "    # 3. add that child to the list of children\n",
    "    node.children.add(new_child)\n",
    "    # 4. return that new child\n",
    "    return new_child\n",
    "\n",
    "\n",
    "def rollout(node, start_state):\n",
    "    winner = node.state.copy().play_random_moves_until_done()\n",
    "    if winner == 0:\n",
    "        return 0\n",
    "    elif winner == start_state.current_player:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def backpropagate(node, simulation_result):\n",
    "    \"\"\"Update the node and its parent (via recursion).\"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32d114",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation Code\n",
    "\n",
    "def print_wins(wins):\n",
    "    print(\"======\")\n",
    "    print(f\"Total Plays: {len(wins)}\")\n",
    "    print(f\"MiniMax Wins: {len([w for w in wins if w == 1])}\")\n",
    "    print(f\"MCTS Wins: {len([w for w in wins if w == 2])}\")\n",
    "    print(f\"Draws: {len([w for w in wins if w == 0])}\")\n",
    "\n",
    "wins = []\n",
    "for _ in range(25):\n",
    "    tot_time_minimax = 0\n",
    "    tot_time_mcts = 0\n",
    "    board = ConnectFourBoard(nrows=6, ncols=7)\n",
    "    board.current_player = random.choice([1, 2])\n",
    "    while board.winner is None:\n",
    "        if board.current_player == 1:\n",
    "            stime = time.time()\n",
    "            action = minimax(board, depth=5, verbose=False)\n",
    "            tot_time_minimax += time.time() - stime\n",
    "        else:\n",
    "            stime = time.time()\n",
    "            action = monte_carlo_tree_search(board)\n",
    "            tot_time_mcts += time.time() - stime\n",
    "        board.play_move(action)\n",
    "    \n",
    "    print(board)\n",
    "    print(f\"Winner: {board.winner}\")\n",
    "    print(tot_time_minimax, tot_time_mcts)\n",
    "    wins.append(board.winner)\n",
    "    print_wins(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f31fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS695 SP22)",
   "language": "python",
   "name": "venv695"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
