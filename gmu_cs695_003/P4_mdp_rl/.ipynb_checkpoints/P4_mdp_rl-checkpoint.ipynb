{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c6bcf8",
   "metadata": {},
   "source": [
    "# P4: MDPs and Reinforcement Learning\n",
    "\n",
    "**DUE: Saturday April 2nd, 2022**\n",
    "\n",
    "**Instructions for submission are the same as they have been.**\n",
    "\n",
    "You may recognize some of the starter code as being from L02! I'm giving you a random adjacency graph for you to play around with. The agent's goal is to maximize it's reward (which happens when it reaches the node in the \"middle\" of the graph). There's a catch: actions are somewhat stochastic. Every time the agent executes an action, there is a random chance it will end up in any of the states reachable from its current state. I am providing you with a couple *policies*: functions that take in a state and return the action the agent should take.\n",
    "\n",
    "This is the environment that you will be using for most of your assignment, so you might want to read through it to make sure you understand what's going on. A few things to note:\n",
    "\n",
    "$P(s' | s, a) = $ `world.get_transition_probs(state, action)`\n",
    "\n",
    "$R(s) = $ `world.rewards`\n",
    "\n",
    "Also, if you have a value function $V(s)$, this means that $\\sum_{s' \\in S} P(s' | s, a) V^*(s')$ = `np.dot(world.get_transition_probs(state, action), values)`. I'm sure this will help you in your answers below!\n",
    "\n",
    "Note that in your implementations, sometimes you want to get the argmax or argmin of some list: you want to find the action that maximizes or minimizes some value. Here's an example: if you want to find the action that minimizes the value of `something`, you can use the following code:\n",
    "```python\n",
    "actions = env.get_actions_for_state(state)\n",
    "action_index = np.argmin(something[actions])\n",
    "minimizing_action = actions[action_index]\n",
    "```\n",
    "Your list of actions acts as a list of indices for the `something` vector, so `something[actions]` gives you the values of `something` for those actions. `np.argmin` gives you the index that maximizes `something[actions]`. With that index, you can plug it back into the list of actions to get the action that would have minimized `something`. I expect you will find this useful for a few aspects of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code: Graph definition and plotting (modified from L02B)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def get_random_adjacency(band_size, num_nodes, edge_rate=1.0, seed=695):\n",
    "    random.seed(seed)\n",
    "    mat = np.zeros((num_nodes, num_nodes))\n",
    "    for ii in range(num_nodes):\n",
    "        mat[ii, ii] = 1.0\n",
    "        for jj in range(ii + 1, min(ii+band_size, num_nodes)):\n",
    "            val = 1.0 * (random.random() < edge_rate)\n",
    "            mat[ii, jj] = val\n",
    "            mat[jj, ii] = val\n",
    "    return mat\n",
    "\n",
    "\n",
    "def plot_adjacency_mat(mat):\n",
    "    plt.figure(dpi=150)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.imshow(mat, interpolation='none')\n",
    "\n",
    "\n",
    "class AdjacencyWorld(object):\n",
    "    def __init__(self, num_nodes, band_size=10, edge_rate=0.5, random_move_chance=0.5, seed=695):\n",
    "        self.mat = get_random_adjacency(band_size=band_size, num_nodes=num_nodes, edge_rate=edge_rate, seed=seed)\n",
    "        self.states = list(range(num_nodes))\n",
    "        self.rewards = -1 * np.ones((num_nodes))\n",
    "        self.goal = num_nodes//2\n",
    "        self.rewards[self.goal] = 10\n",
    "        self.random_move_chance = random_move_chance\n",
    "    \n",
    "    def get_actions_for_state(self, state): \n",
    "        \"\"\"Get the available 'moves' from the current state.\n",
    "        return: list of indices connected to 'state'.\"\"\"\n",
    "        return np.where(self.mat[state])[0].tolist()\n",
    "    \n",
    "    def get_transition_probs(self, state, action):\n",
    "        \"\"\"Get a list of the transition probabilities for a state and action.\"\"\"\n",
    "        actions = self.get_actions_for_state(state)\n",
    "        num_actions = len(actions)\n",
    "        prob_vec = np.zeros_like(self.rewards)\n",
    "        for rand_action in actions:\n",
    "            prob_vec[rand_action] = self.random_move_chance / num_actions\n",
    "        \n",
    "        prob_vec[action] += (1 - self.random_move_chance)\n",
    "        return prob_vec\n",
    "    \n",
    "    def get_random_state(self):\n",
    "        return random.choice(self.states)\n",
    "\n",
    "    def execute_action(self, state, action):\n",
    "        assert action in self.get_actions_for_state(state)\n",
    "        probs = self.get_transition_probs(state, action)\n",
    "        new_state = np.random.choice(self.states, p=probs)\n",
    "        return self.rewards[new_state], new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba1601",
   "metadata": {},
   "source": [
    "## P4.1 Evaluating Policies\n",
    "\n",
    "In class, we discussed two different ways of evaluating a policy: First-visit MC prediction and a linear-algebra-based procedure.\n",
    "\n",
    "### P4.1.1 First-Visit MC Policy Evaluation [4 points]\n",
    "\n",
    "**TASK** Complete the `evaluate_policy_first_visit_mc` function below using the First Visit MC prediction algorithm we discussed in class. \n",
    "\n",
    "I have provided you with significant starter code for this purpose. At each step during the MC evaluation, I record the current state and action, the reward returned after executing the action, and the states that have been visited so far (needed to evaluate whether it was a *first visit*). In the second loop, which you should complete, we iterate through that list backwards, as it is done in the algorithm pseudocode from class. It might also help you to know that I have stored the returns as a python dictionary; when you want to add a new 'return', you should use something that looks like `returns[state] += [something]`. Once you have populated the returns, computing the values can be done by looping through `state, returns in returns.items()`; feel free to use a value of 0 if `len(returns) == 0`.\n",
    "\n",
    "When completed, if implemented correctly, for a \"random move chance\" of 0.2, the average value of policy 1 should be roughly -48 and the average return value of policy 2 should be roughly 20 (plus or minus three or so).\n",
    "\n",
    "**CODE** Include your completed implementation in your writeup.\n",
    "\n",
    "**RESULTS** Include the computed output average values for each of the provided policies.\n",
    "\n",
    "**PLOTS** Include the generated plot of the computed value for each policy (for a random move chance of 0.5) in your writeup.\n",
    "\n",
    "**QUESTION** (1-2 sentences) How do the costs of each policy compare as the random move chance becomes 1.0? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ec876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STARTER CODE: First Visit MC Policy Evaluation\n",
    "\n",
    "def evaluate_policy_first_visit_mc(world, policy, num_iterations=1000, gamma=0.98, num_steps=200, seed=695):\n",
    "    \"\"\"Returns a list such that value[state] is the value of that state.\"\"\"\n",
    "    # Perform a bunch of rollouts\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    returns = {s: [] for s in world.states}\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Rollout\n",
    "        state = world.get_random_state()\n",
    "        steps = []\n",
    "        states_so_far = set()\n",
    "        for _ in range(num_steps):\n",
    "            action = policy[state]\n",
    "            reward, new_state = world.execute_action(state, action)\n",
    "            steps.append((state, action, reward, states_so_far.copy()))\n",
    "            states_so_far.add(new_state)\n",
    "            state = new_state\n",
    "        \n",
    "        G = 0\n",
    "        for s, a, r, states_so_far in reversed(steps):\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    # Return the values\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and evaluation code\n",
    "world = AdjacencyWorld(200, band_size=20, edge_rate=0.1, random_move_chance=0.0, seed=695)\n",
    "\n",
    "\n",
    "# Policy 1\n",
    "policy_1 = [random.choice(world.get_actions_for_state(state)) \n",
    "            for state in world.states]\n",
    "\n",
    "# Policy 2\n",
    "policy_2 = [0] * len(world.states)\n",
    "for state in world.states:\n",
    "    actions = np.array(world.get_actions_for_state(state))\n",
    "    action_ind = np.argmin(np.abs(actions - world.goal))\n",
    "    policy_2[state] = actions[action_ind]\n",
    "    \n",
    "def evaluate_policy_multiple(world, policy):\n",
    "    for rmc in [0.0, 0.2, 0.5, 0.8, 1.0]:\n",
    "        world.random_move_chance = rmc\n",
    "        values = evaluate_policy_first_visit_mc(world, policy)\n",
    "        print(f\" Chance: {rmc:4.1f} | Average Value: {np.mean(values)}\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "world.random_move_chance = 0.5\n",
    "plt.plot(evaluate_policy_first_visit_mc(world, policy_1), 'r*')\n",
    "plt.plot(evaluate_policy_first_visit_mc(world, policy_2), 'k.')\n",
    "\n",
    "print(\"== Policy 1 ==\")\n",
    "evaluate_policy_multiple(world, policy_1)\n",
    "\n",
    "print(\"== Policy 2 ==\")\n",
    "evaluate_policy_multiple(world, policy_2)\n",
    "\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde14ac",
   "metadata": {},
   "source": [
    "### P4.1.2 Linear Algebra Solution [4 points]\n",
    "\n",
    "As we discussed in class, there is a more direct (non-sampling-based) way to tackle this problem: linear algebra. While the strategy doesn't scale well to large problems, ours is small enough that it won't matter much. This method aims to solve the following linear system:\n",
    "\n",
    "$$ V(s) = R_s + \\gamma \\sum_{s'} P_(s' | s, a)V(s') $$\n",
    "\n",
    "In matrix, form we can write this as follows:\n",
    "\n",
    "$$ V = R + \\gamma P V $$\n",
    "\n",
    "Rearranging, we find:\n",
    "\n",
    "$$ (\\mathbb{I} - \\gamma P) V = R $$\n",
    "\n",
    "where $\\mathbb{I}$ is the identity matrix (implemented in numpy as `np.eye(N)`, where N is the side length). Numpy can be used to solve for a linear system like this. If $Ax = b$, then `x = np.linalg.solve()`. It is up to you to figure out how to build the $P$ matrix using `world.get_transition_probs(state, action)` (where `action = policy[state]` by definition of the policy).\n",
    "\n",
    "Your results should be pretty close to the previous implementation, though the computation will no doubt be significantly faster.\n",
    "\n",
    "**TASK** Complete the `evaluate_policy` function below.\n",
    "\n",
    "**CODE** Include the completed `evaluate_policy` function in your writeup.\n",
    "\n",
    "**RESULTS** Include the computed output average values for each of the provided policies.\n",
    "\n",
    "**PLOTS** Include the generated plot of the computed value for each policy (for a random move chance of 0.5) in your writeup.\n",
    "\n",
    "**QUESTION** (1-2 sentences) How do the costs of each policy compare as the random move chance becomes 1.0? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma=0.98):\n",
    "    \"\"\"Returns a list of values[state].\"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_multiple(world, policy):\n",
    "    for rmc in [0.0, 0.2, 0.5, 0.8, 1.0]:\n",
    "        world.random_move_chance = rmc\n",
    "        values = evaluate_policy(world, policy)\n",
    "        print(f\" Chance: {rmc:4.1f} | Average Value: {np.mean(values)}\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "world.random_move_chance = 0.5\n",
    "plt.plot(evaluate_policy(world, policy_1), 'r*')\n",
    "plt.plot(evaluate_policy(world, policy_2), 'k.')\n",
    "\n",
    "print(\"== Policy 1 ==\")\n",
    "evaluate_policy_multiple(world, policy_1)\n",
    "\n",
    "print(\"== Policy 2 ==\")\n",
    "evaluate_policy_multiple(world, policy_2)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfda55",
   "metadata": {},
   "source": [
    "## P4.2 Value Iteration [8 points]\n",
    "\n",
    "In class, we looked at *Value Iteration* as a technique to compute the *optimal policy*. Here, you will implement two functions relevant to this problem: value iteration itself, and a separate function that is used to determine the optimal policy from the computed values and the world model. First, you will implement value iteration.\n",
    "\n",
    "**TASK** Implement the `value_iteration` function below using information from the in-class slides for how to iteratively update the value function. Key to this is that you are trying to find the action which will maximize the future expected reward. For this purpose, you will need both `world.get_actions_for_state(state)` and `world.get_transition_probs(state, action)`.\n",
    "\n",
    "**CODE** Include your implementation of `compute_policy_from_values` in your writeup.\n",
    "\n",
    "Next you will need to compute the policy from the values you computed above. For our scenario, to compute the policy from values, you should use the following formula.\n",
    "\n",
    "$$ \\pi^*(s) = \\arg\\max_a \\sum_{s' \\in S} P(s' | s, a) V^*(s') $$\n",
    "\n",
    "**TASK** Implement the `compute_policy_from_values` function.\n",
    "\n",
    "**CODE** Include your implementation of `compute_policy_from_values` in your writeup.\n",
    "\n",
    "**RESULTS & PLOTS** Finally, run the code below and include the plots and results.\n",
    "\n",
    "**QUESTION** (2-3 sentences) There are a few states for which the value is very low for all three values of random move chance. Why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code\n",
    "\n",
    "def value_iteration(world, num_iterations, gamma=0.98):\n",
    "    # values is a vector containing the value for each state.\n",
    "    values = world.rewards.copy()\n",
    "    # values_over_iterations is a vector of the values at each iteration (for visualization)\n",
    "    values_over_iterations = [values.copy()]\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Perform one step of value iteration\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Store the values in the 'all_values' list (for visualization)\n",
    "        values_over_iterations.append(values.copy())\n",
    "        \n",
    "    # Return the all_values list; all_values[-1] are the final values.\n",
    "    return values_over_iterations\n",
    "\n",
    "\n",
    "def compute_policy_from_values(world, values):\n",
    "    \"\"\"policy is a mapping from states -> actions. \n",
    "    Here, it's just a vector: action = policy[state]\"\"\"\n",
    "    # Initialize the policy vector\n",
    "    policy = np.zeros_like(world.states)\n",
    "\n",
    "    # Compute the policy for every state\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results & Plotting Code\n",
    "\n",
    "# Plot the adjacency matrix we'll be using\n",
    "world = AdjacencyWorld(200, band_size=20, edge_rate=0.1, random_move_chance=0.0, seed=695)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.subplot(121)\n",
    "plt.title(\"Policies (VI)\")\n",
    "ax2 = plt.subplot(122)\n",
    "plt.title(\"Values (VI)\")\n",
    "\n",
    "# No randomness in move selection\n",
    "world.random_move_chance=0.0\n",
    "all_values = value_iteration(world, 100)\n",
    "policy_vi_00 = compute_policy_from_values(world, all_values[-1])\n",
    "ax1.plot(policy_vi_00, 'g^')\n",
    "ax2.plot(all_values[-1], 'g^')\n",
    "\n",
    "# Some randomness in move selection\n",
    "world.random_move_chance=0.4\n",
    "all_values = value_iteration(world, 100)\n",
    "policy_vi_02 = compute_policy_from_values(world, all_values[-1])\n",
    "ax1.plot(policy_vi_02, 'b*')\n",
    "ax2.plot(all_values[-1], 'b*')\n",
    "\n",
    "# A lot of randomness in move selection\n",
    "world.random_move_chance=0.8\n",
    "all_values = value_iteration(world, 100)\n",
    "policy_vi_08 = compute_policy_from_values(world, all_values[-1])\n",
    "ax1.plot(policy_vi_08, 'r.')\n",
    "ax2.plot(all_values[-1], 'r.')\n",
    "\n",
    "# Visualize the evolution of the values over time\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_adjacency_mat(all_values)\n",
    "plt.title(\"Values vs Iterations (VI, RMC=0.8)\")\n",
    "\n",
    "\n",
    "print(\"== Policy 1 ==\")\n",
    "evaluate_policy_multiple(world, policy_1)\n",
    "\n",
    "print(\"== Policy 2 ==\")\n",
    "evaluate_policy_multiple(world, policy_2)\n",
    "\n",
    "print(\"== Policy Value Iteration (rmc = 0.2) ==\")\n",
    "evaluate_policy_multiple(world, policy_vi_02)\n",
    "\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e282c8",
   "metadata": {},
   "source": [
    "## P4.3 Policy Iteration [8 points]\n",
    "\n",
    "Finally, you will implement policy iteration and compare the results to value iteration. The procedure (see lecture slides) involves iteratively computing the values for the given policy and then re-computing the best policy given the current estimate of the values.\n",
    "\n",
    "**TASK** Implement \"full\" Policy Iteration by completing `policy_iteration` below. Note that you already have most of the pieces you need to complete this function! You already know how to use linear algebra to compute the values given a policy *and* you already know how to compute the \"best\" action (policy) given an estimate of the values. Feel free to use functions you have already implemented in your response.\n",
    "\n",
    "**CODE** Include your implementation in your writeup.\n",
    "\n",
    "**PLOTS** Include the generated plots in your writeup.\n",
    "\n",
    "**QUESTION** (2-4 sentences) Which approach converges more quickly? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code\n",
    "\n",
    "def policy_iteration(world, num_iterations, gamma=0.99):\n",
    "    values = world.rewards.copy()\n",
    "    all_values = []\n",
    "    all_values.append(values.copy())\n",
    "    # Get random policy\n",
    "    policy = [random.choice(world.get_actions_for_state(state)) \n",
    "              for state in world.states]\n",
    "    \n",
    "    for ii in range(num_iterations):\n",
    "        # Update the values (using solution)\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Update the policy\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        all_values.append(values.copy())\n",
    "\n",
    "        # Terminate (break) if the policy does not change between steps\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return policy, all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of randomness in move selection\n",
    "import time\n",
    "world = AdjacencyWorld(200, band_size=20, edge_rate=0.1, random_move_chance=0.5)\n",
    "\n",
    "stime = time.time()\n",
    "all_values_vi = value_iteration(world, 100)\n",
    "policy_vi = compute_policy_from_values(world, all_values[-1])\n",
    "print(f'Value Iteration Time: {time.time() - stime}')\n",
    "\n",
    "stime = time.time()\n",
    "policy_pi, all_values_pi = policy_iteration(world, 100)\n",
    "print(f'Policy Iteration Time: {time.time() - stime}')\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(policy_vi, 'r*')\n",
    "plt.plot(policy_pi, 'k.')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(all_values_vi)\n",
    "plt.title(\"Values vs Iterations (VI)\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(all_values_pi)\n",
    "plt.title(\"Values vs Iterations (PI)\")\n",
    "\n",
    "pv = evaluate_policy(world, policy_vi)\n",
    "print(f\"Value Iteration Avg. Value: {np.mean(pv)}\")\n",
    "pv = evaluate_policy(world, policy_pi)\n",
    "print(f\"Policy Iteration Avg. Value: {np.mean(pv)}\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d26815",
   "metadata": {},
   "source": [
    "## P4.4: Q Learning [8 points]\n",
    "\n",
    "Finally, you'll be implementing Q Learning. Instructions here are a bit sparse, but I figure you probably know what you're doing by now!\n",
    "\n",
    "**TASK** Implement Q Learning (use the lecture slides as a reference).\n",
    "\n",
    "**CODE** Include your Q Learning implementation in your writeup\n",
    "\n",
    "**PLOTS** Run the evaluation code below. Include all plots and the results (the average value for each value of the learning rate) in your writeup. You should notice the performance peaks around a learning rate of 0.02.\n",
    "\n",
    "**QUESTION** (1-3 sentences) The rate of convergence for Q learning is significantly slower than that of Value Iteration. What information does Value Iteration have access to (and indeed makes use of) that makes it converge faster?\n",
    "\n",
    "**QUESTION** (3-5 sentences) When the learning rate is very low, the performance is not particularly good. From looking at the plots of the total reward over time, what is likely the cause? How would you fix this issue (without changing the learning rate)?\n",
    "\n",
    "**QUESTION** (2-4 sentences) When the learning is too high, the performance is also not very good. Why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed525c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code: Q Learning\n",
    "\n",
    "def Q_learning(env, num_iterations, num_steps=30, gamma=0.98,\n",
    "               learning_rate=0.005, epsilon=0.1, seed=695):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    Q_s_a = np.zeros((len(env.states), len(env.states)))\n",
    "    total_rewards = []\n",
    "    # Iterate\n",
    "    for ii in range(num_iterations):\n",
    "        # Rollout\n",
    "        total_reward = 0\n",
    "        state = env.get_random_state()\n",
    "        for ii in range(num_steps):\n",
    "            # Take an action and get the reward\n",
    "            actions = env.get_actions_for_state(state)\n",
    "            if random.random() > epsilon:\n",
    "                action_ind = np.argmax(Q_s_a[state, actions])\n",
    "                action = actions[action_ind]\n",
    "            else:\n",
    "                action = random.choice(actions)\n",
    "            r, new_state = env.execute_action(state, action)\n",
    "            \n",
    "            # Update Q\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Update reward and state\n",
    "            total_reward += r\n",
    "            state = new_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    policy = np.zeros(len(env.states))\n",
    "    for state in env.states:\n",
    "        actions = env.get_actions_for_state(state)\n",
    "        action_ind = np.argmax(Q_s_a[state, actions])\n",
    "        policy[state] = actions[action_ind]\n",
    "    return list(policy.astype(int)), total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add03db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "\n",
    "world = AdjacencyWorld(250, band_size=20, edge_rate=0.4, random_move_chance=0.3)\n",
    "\n",
    "def smooth_data(data, window_width=50):\n",
    "    summed_data = np.cumsum(data)\n",
    "    sm_data = (summed_data[window_width:] - summed_data[:-window_width]) / window_width\n",
    "    return np.concatenate([np.zeros(window_width), sm_data])\n",
    "\n",
    "\n",
    "def evaluate_q_learning(learning_rate):\n",
    "    policy_ql, rewards = Q_learning(world, num_iterations=4000,\n",
    "                                    learning_rate=learning_rate)\n",
    "    pv = evaluate_policy(world, policy_ql)\n",
    "    print(f\"Q Learning Avg. Value ({learning_rate}): {np.mean(pv)}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(policy_pi, 'r*')\n",
    "    plt.plot(policy_ql, 'k.')\n",
    "    plt.title('Policy')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(rewards, alpha=0.5)\n",
    "    plt.plot(smooth_data(rewards))\n",
    "    plt.title(f'Reward over Time (lr={learning_rate})')\n",
    "\n",
    "\n",
    "policy_pi, _ = policy_iteration(world, num_iterations=100, gamma=0.98)\n",
    "pv = evaluate_policy(world, policy_pi)\n",
    "print(f\"Policy Iteration Avg. Value: {np.mean(pv)}\")\n",
    "\n",
    "evaluate_q_learning(learning_rate=0.001)\n",
    "evaluate_q_learning(learning_rate=0.005)\n",
    "evaluate_q_learning(learning_rate=0.02)\n",
    "evaluate_q_learning(learning_rate=0.10)\n",
    "evaluate_q_learning(learning_rate=1.00)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b285806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS695 SP22)",
   "language": "python",
   "name": "venv695"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
