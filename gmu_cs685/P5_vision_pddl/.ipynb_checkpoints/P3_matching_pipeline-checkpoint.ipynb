{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3 Matching Pipeline\n",
    "\n",
    "## Due: Sunday, October 10th, 2021\n",
    "\n",
    "**Note: If you cannot get any parts of this assignment working, you should include (small) blocks of partially implemented code in your writeup; we will use this for evaluating partial credit.**\n",
    "\n",
    "The instructions are the same as for the previous programming assignments, so I expect you know the procedure by now. I will ask you to submit both the PDF of your writeup *and* a .zip file containing your code (which may be a modified form of this Jupyter notebook) on Blackboard. Though it would be nice if your code were cleaned up and nicely formatted/commented, we will not likely execute your code unless we are skeptical for academic integrity reasons. We may also use your code to evaluate partial credit on multi-part assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3.1 Scaling and Rotating Patches\n",
    "\n",
    "In the previous programming assignment, we took a look at a few different feature descriptors. Each had their advantages and disadvantages, yet none were particularly good at matching features that had changed in both scale and rotation. In this question, I will walk you through a process for more effective feature descriptors that compensate for both.\n",
    "\n",
    "### P3.1.1 Scaling and Rotating Features: Concepts\n",
    "\n",
    "The first thing we will need is the ability to compute an image patch corresponding to the feature. To do that, the image patch will need to translate the images, rotate them, and scale them.\n",
    "Fortunately, in the last assignment, you were asked to write code that transformed an image using a general homography matrix $H$. First, a conceptual question:\n",
    "\n",
    "**(QUESTION)** If I have a feature located at $(x_f, y_f)$ with orientation $\\theta$ and radius (\"scale\") $s$, what is the transformation matrix $H$ that simultaneously moves the feature to the origin, un-rotates it, and un-scales it (so that the feature becomes 1 pixel wide)?\n",
    "\n",
    "For example, if I had a feature that was already at the origin, and not rotated, but was scaled such that it's radius was 10 pixels wide, the transformation matrix would need to make the feature smaller, so $H$ would be defined as:\n",
    "\n",
    "$$H = \\begin{bmatrix}\n",
    "  1/10 & 0 & 0 \\\\\n",
    "  0 & 1/10 & 0 \\\\\n",
    "  0 & 0 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "*Note*: Because of the challenges involved with intuiting the direction of the transformation, I will accept either the transformation I have described, or its inverse (which you will likely need for the next part of this question).\n",
    "\n",
    "### P3.1.2 Scaling and Rotating Features: Implementation\n",
    " \n",
    "The inverse of the transformation matrix I have asked for above can be used to compute an image patch surrounding a feature that compensates for both the scale and the orientation of that feature. The computed patches can then be used as feature descriptors for feature matching to align images. In this part of the question, you will implement the warping function to compute these patches. I have provided you with starter code in the function `get_scaled_rotated_patch` below. Missing is the transformation matrix, which can be implemented using your solution to the previous part of this question. I have used `scipy.interpolate` to implement the interpolation in the warping loop itself; you should feel free to use this implementation.\n",
    "\n",
    "> The purpose of this function is to compute a square patch of pixels that are centered around a pixel and can compensate for the scale and rotation of the feature. To show that it is working, you should generate the figure below, and it should look identical to the image I have provided! The function specification I have laid out below (in the Python code) is rather complicated, so *some* trial and error is expected, but brute-forcing the solution is not advised.\n",
    "\n",
    "I have included some sample code following the `get_scaled_rotated_patch` function that generates some image patches for various parameters on a reference image. If your transformation is implemented correctly your figure should look as follows:\n",
    "\n",
    "<img src=\"get_patch_examples_b.png\" width=\"400\">\n",
    "\n",
    "**FIGURE**: To demonstrate that your code is working, change the following parameters `base_center_x = 500` and `base_center_y = 640` and regenerate the figure. Include this new figure in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter Code: Getting Image Patches\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "\n",
    "def get_scaled_rotated_patch(\n",
    "    image,\n",
    "    feature_center_x,\n",
    "    feature_center_y,\n",
    "    feature_radius,\n",
    "    patch_radius,\n",
    "    feature_orientation,\n",
    "    half_pixel_width):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - image :: the image from which the patch is computed\n",
    "    - feature_center_x :: image-space pixel coord of x-center of feature\n",
    "    - feature_center_y :: image-space pixel coord of y-center of feature\n",
    "    - feature_radius :: radius of feature (in num of pixels in image)\n",
    "    - patch_radius :: desired \"radius\" of patch (also in num of pixels in image)\n",
    "    - feature_orientation :: orientation of the feature\n",
    "    - half_pixel_width :: controls patch size (see details below)\n",
    "    \n",
    "    Some details:\n",
    "    - The patch radius and the feature radius are both in the same coordinates.\n",
    "      this means that, if one desires that if one wants a patch to contain a\n",
    "      feature of radius 10 pixels (feature_radius=10), setting patch_radius=10\n",
    "      would result in a patch where the feature touched the borders of the patch.\n",
    "      If one instead set patch_radius=20, the feature would be half as wide as\n",
    "      the patch and located at its center.\n",
    "    - The `half_pixel_width` determines the patch size. The width and height of the\n",
    "      patch are equal to `2*half_pixel_width + 1`. This means that setting\n",
    "      half_pixel_width=4, would result in a 9x9 patch.\n",
    "    - The orientation is the computed orientation *of the feature itself*. The\n",
    "      rotation that is performed to compute the patch is to compensate for this\n",
    "      rotation, and might be negative of what you expect.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the pixel-space vectors for the input image\n",
    "    # Note: do not modify these\n",
    "    x = np.arange(image.shape[1]).astype(np.float)\n",
    "    y = np.arange(image.shape[0]).astype(np.float)\n",
    "    \n",
    "    # The patch has a different domain from the original image.\n",
    "    # Compute the vectors for the patch coordinates\n",
    "    # Note: do not modify these either...\n",
    "    patch_width = 2*half_pixel_width+1\n",
    "    s = patch_radius/feature_radius\n",
    "    xi = np.linspace(-s, s, patch_width)\n",
    "    yi = np.linspace(-s, s, patch_width)\n",
    "\n",
    "    # Apply the transformations to get the new coordinates\n",
    "    # (Again, we flip the coordinates due to the convention difference\n",
    "    # between [rows, columns] and [x, y].)\n",
    "    transformation_matrix = None\n",
    "    if transformation_matrix is None:\n",
    "        raise NotImplementedError(\"Define the transformation matrix.\")\n",
    "\n",
    "    # Perform the transformation+interpolation\n",
    "    patch = np.zeros((patch_width, patch_width))\n",
    "    image_fn = scipy.interpolate.interp2d(x, y, image, fill_value=0)\n",
    "    for ii in range(len(xi)):\n",
    "        for jj in range(len(yi)):\n",
    "            new_x = None\n",
    "            new_y = None\n",
    "            if new_x is None or new_y is None:\n",
    "                raise NotImplementedError(\n",
    "                    \"You must define the new coordinates\")\n",
    "            patch[jj, ii] = image_fn(new_x, new_y)\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for generating the figure of patches (do not change).\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate\n",
    "import scipy.signal\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return (np.asarray(img).astype(np.float)/255)[:, :, :3]\n",
    "\n",
    "image = load_image(\"light_cubes_base.png\")[:, :, 0]\n",
    "\n",
    "# Values for the reference figure above\n",
    "base_center_x = 800\n",
    "base_center_y = 600\n",
    "\n",
    "# # You will use these values for your submitted figure.\n",
    "# base_center_x = 500\n",
    "# base_center_y = 640\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Loop over radius\n",
    "for ind, rad in enumerate([25, 50, 75, 100]):\n",
    "    plt.subplot(4, 4, 1+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=rad,\n",
    "        patch_radius=2*rad,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=20),\n",
    "              vmin=0, vmax=1)\n",
    "\n",
    "# Loop over orientation\n",
    "# (Remember that orientation is of the original feature)\n",
    "for ind, th_deg in enumerate([0, 15, 30, 45]):\n",
    "    th_rad = np.pi * th_deg / 180\n",
    "    plt.subplot(4, 4, 5+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=th_rad,\n",
    "        half_pixel_width=20))\n",
    "\n",
    "    \n",
    "# Loop over location\n",
    "for ind, x_shift in enumerate([-50, 0, 50, 100]):\n",
    "    plt.subplot(4, 4, 9+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x + x_shift,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=20))\n",
    "    \n",
    "# Loop over resolution\n",
    "# (Remember that orientation is of the original feature)\n",
    "for ind, hpw in enumerate([10, 20, 40, 60]):\n",
    "    plt.subplot(4, 4, 13+ind)\n",
    "    plt.imshow(get_scaled_rotated_patch(\n",
    "        image=image,\n",
    "        feature_center_x=base_center_x,\n",
    "        feature_center_y=base_center_y,\n",
    "        feature_radius=100,\n",
    "        patch_radius=2*100,\n",
    "        feature_orientation=0.0,\n",
    "        half_pixel_width=hpw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3.2 Computing Homographies from Matches\n",
    "\n",
    "\n",
    "### P3.2.1 Computing Homographies from Perfect Matches\n",
    "\n",
    "In this problem, you will be computing homographies from feature matches that you generate by hand. This involves two steps:\n",
    "\n",
    "1. Obtain feature matches between the two images. *For this question will be \"computing\" these matches by hand (you will do this automatically in another question). Most operating systems have an image inspection program that allows you to quickly get the coordinates of a pixel. Alternatively, you can use trial and error with the `visualize_matches` function I have provided for your convenience below.*\n",
    "2. Compute the homography matrix $H$ using the procedure introduced in class. This means that you are trying to find a matrix $A$ that satisfies the following relation:\n",
    "\n",
    "<img src=\"homography_slide_a.png\" width=\"400\">\n",
    "<img src=\"homography_slide_b.png\" width=\"400\">\n",
    "\n",
    "In the code block labeled `An example of match visualization` below, I have given you a full worked example of what this process will look like: I have generated an example transformed image using a known homography `H_known`, provided some `matches`, used those matches to compute the homography `H_computed` (using a function you will write), and then visualized the results using `visualize_computed_transform`. This is what a \"correct solution\" should look like.\n",
    "\n",
    "**FIGURES** I have provided you with 6 images: `img_base` (the starter image in the code below), and 5 \"transformed images\", each with different homography matrices. For each \"transformed image\", manually identify at least four matches between it and the base image, use these matches to compute the homography $H$ and use the `visualize_compute_transform` function to generate a plot. Include these plots in your writeup.\n",
    "\n",
    "Your \"reconstruction difference\" plots are not expected to be perfect, but should be reasonably close to accurate; you will not be penalized for small differences; I have provided an example below to show this. If the reconstructed image is completely different from the base image, you will be marked as incorrect. (Also note that if the transformed image is \"missing\" part of the original image due to cropping from the transform, you can ignore those regions as well.) It may also help to use more than 4 matches when computing the transforms, since this will help to reduce the impact of small errors on the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting and Helper Functions\n",
    "def visualize_matches(img_a, img_b, matches, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    sa = img_a.shape\n",
    "    sb = img_b.shape\n",
    "    sp = 40\n",
    "    off = sa[1]+sp\n",
    "    \n",
    "    merged_imgs = np.zeros(\n",
    "        (max(sa[0], sb[0]), sa[1]+sb[1]+sp),\n",
    "        dtype=np.float)\n",
    "    merged_imgs[0:sa[0], 0:sa[1]] = img_a\n",
    "    merged_imgs[0:sb[0], sa[1]+sp:] = img_b\n",
    "    ax.imshow(merged_imgs)\n",
    "    \n",
    "    for m in matches:\n",
    "        ax.plot([m[0], m[2]+off], [m[1], m[3]], 'r', alpha=0.5)\n",
    "\n",
    "\n",
    "def transform_image(image, tmat):\n",
    "    import cv2\n",
    "    return cv2.warpPerspective(\n",
    "        image, \n",
    "        np.array(tmat).astype(float), \n",
    "        dsize=(image.shape[1], image.shape[0]))\n",
    "\n",
    "def visualize_computed_transform(image_base, image_transformed, H, matches):\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=150)\n",
    "    tmat = np.linalg.inv(H)\n",
    "    image_rec = transform_image(image_transformed, tmat)\n",
    "    \n",
    "    # Plotting\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    visualize_matches(image_base, image_transformed, matches, ax)\n",
    "    plt.title('Base Images (with matches)')\n",
    "    ax = plt.subplot(2, 2, 3)\n",
    "    plt.imshow(image_rec, vmin=0, vmax=1)\n",
    "    plt.title('Reconstructed Image')\n",
    "    ax = plt.subplot(2, 2, 4)\n",
    "    plt.imshow(image_base - image_rec, vmin=-0.3, vmax=0.3, cmap='PiYG')\n",
    "    plt.title('Reconstruction Difference')\n",
    "    \n",
    "# Load the Images\n",
    "#  Base Image\n",
    "img_base = load_image('tr_base.png')[:, :, 0]\n",
    "#  Transformed Images\n",
    "img_tr = load_image('tr_translated.png')[:, :, 0]\n",
    "img_ro = load_image('tr_rotated.png')[:, :, 0]\n",
    "img_sa = load_image('tr_aspect_scaling.png')[:, :, 0]\n",
    "img_ha = load_image('tr_homography_a.png')[:, :, 0]\n",
    "img_hb = load_image('tr_homography_b.png')[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of match visualization\n",
    "\n",
    "H_known = [\n",
    "    [1.2, 0, 0],\n",
    "    [0, 1.2, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "img_example = transform_image(img_base, H_known)\n",
    "\n",
    "# Matches Stored: [x1, y1, x2, y2]\n",
    "# I computed these by inspection.\n",
    "matches = [\n",
    "    [30, 125, 36, 150],\n",
    "    [20, 30, 25, 35],\n",
    "    [80, 120, 96, 144],\n",
    "    [220, 90, 264, 108],\n",
    "]\n",
    "# You will be computing this yourself using your implementation\n",
    "# of the `solve_homography` function.\n",
    "H_computed = [\n",
    "    [ 1.21083264e+00, -8.97707425e-03,  1.11129596e+00],\n",
    "    [ 3.15219064e-03,  1.22310850e+00, -1.67420761e+00],\n",
    "    [ 1.96243539e-05,  6.50992714e-05,  1.00000000e+00]]\n",
    "visualize_computed_transform(\n",
    "    img_base, img_example, H_computed, matches)\n",
    "\n",
    "# This is what a \"correct\" H matrix looks like. In the region\n",
    "# of the \"Reconstruction Difference\" where the transformed\n",
    "# image was in view, the reconstruction is relatively small.\n",
    "# Outside of that range, we do not know what the image looked\n",
    "# like, and the reconstructed image is set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3.2.2 Computing Homographies from Noisy Matches\n",
    "\n",
    "Now, I will ask you to compute the homography of a transform from a set of matches, where some of the matches are \"outliers\". The idea is that you will need to use RANSAC to compute which samples are inliers and which are outliers.\n",
    "\n",
    "I have provided you with two sample images below (in `Noisy Matches Base Code`), and a set of matches. In the plot I have generated, you can see that though many of the matches are correct, there are a few outliers that will ruin the computation of the homography.\n",
    "\n",
    "**FIGURE** Compute the homography with **all** of the `matches_noisy` I have provided and visualize using `visualize_computed_transform`. The resulting transform should be quite poor. Include this plot in your writeup.\n",
    "\n",
    "Next, you will be implementing the RANSAC procedure we discussed in class and use it to compute a homography that is robust to the outlying detections. **Implement RANSAC procedure from class by finishing the `get_inliers` function below** and use this in combination with your `solve_homography` function to compute the homography despite outliers in `matches_noisy`; you are recommended to use the code we worked through in class as a starting point to understand how this will work. You will need a function that computes the inliers from the set of all matches and a proposed transformation matrix $H$. `matches_noisy` has a 10% outlier ratio. You should call your function `solve_homography_ransac(matches)`. You will need it again later. **Please include a code block containing your implementation of the get_inliers part of the RANSAC procedure in your report; it will help us give partial credit in the event that it does not appear to be working correctly.**\n",
    "\n",
    "**FIGURE** Using `visualize_computed_transform`, visualize the transform you have computed using RANSAC (via `solve_homography_ransac`) and `matches_noisy`. Your solution should be quite accurate. If most of your `reconstruction_difference` plot is non-zero, something is probably wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Noisy Matches Base Code\n",
    "img_base = load_image('light_cubes_base.png')[:, :, 0]\n",
    "img_transformed = load_image('light_cubes_transformed.png')[:, :, 0]\n",
    "matches_noisy = np.load('light_cubes_transformed_matches.npy')\n",
    "\n",
    "visualize_matches(img_base, img_transformed, matches_noisy)\n",
    "\n",
    "## RANSAC Solution\n",
    "\n",
    "def solve_homography_ransac(matches, rounds=100, sigma=5, s=4):\n",
    "    num_inliers = 0\n",
    "    best_inliers = []\n",
    "    best_H = []\n",
    "    \n",
    "    def get_inliers(matches, H, dist=sigma, chsq_thresh=5.99):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    for _ in range(rounds):\n",
    "        ps = np.random.choice(np.arange(matches.shape[0]), size=s)\n",
    "        ms = matches[ps]\n",
    "        H = solve_homography(ms)\n",
    "        inliers = get_inliers(matches, H, sigma)\n",
    "        if len(inliers) > len(best_inliers):\n",
    "            best_inliers = inliers\n",
    "            best_H = H.copy()\n",
    "\n",
    "    best_H = solve_homography(best_inliers)\n",
    "    best_inliers = get_inliers(matches, best_H, sigma)\n",
    "\n",
    "    return best_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using your function, compute the homography and plot.\n",
    "H_robust = solve_homography_ransac(matches_noisy, rounds=100)\n",
    "print(f\"Computed Homography: {H_robust}\")\n",
    "\n",
    "visualize_computed_transform(\n",
    "    img_base, img_example, H_robust, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature Matching Pipeline\n",
    "\n",
    "I have provided you with a function `compute_features_with_descriptors` which, well, computes features and their descriptors (via the `compute_scaled_rotated_patch` code you wrote earlier). However, the function is not quite complete, since it still needs a multi-scale feature detector. Fortunately, you wrote one of those in your last assignment:\n",
    "\n",
    "**TASK** Define the function `compute_multi_scale_features` I have created in the `Code you need to provide` below. You can do this with the code you wrote for your last assignment. Notice that I have provided you with a `Feature` class in the code below. The `compute_multi_scale_features` function is expected to return a list of these `Feature` objects for the remainder of the code to work as expected.\n",
    "\n",
    "To confirm that you are computing feature patches and orienting and scaling them correctly, it might be worth visualizing them (though you do not need to include these in your writeup). An example code snippet might look like:\n",
    "\n",
    "```python\n",
    "## Visualize Patches\n",
    "sigmas = np.arange(5, 40.0, 1)\n",
    "image = load_image('light_cubes_base.png')[::1, ::1, 0]\n",
    "features = compute_features_with_descriptors(image, sigmas, 0.6)\n",
    "\n",
    "# Plot a few of the feature patches for your own reference\n",
    "# You should see that they are all aligned.\n",
    "plt.figure()\n",
    "for ind, f in enumerate(mfeatures[:9]):\n",
    "    plt.subplot(3, 3, ind+1)\n",
    "    plt.imshow(f.descriptor)\n",
    "```\n",
    "\n",
    "**TASK** Finally, implement the function `compute_feature_matches(fsa, fsb)`, which returns a list of matched feature pairs `[fa, fb]` from two lists of `Feature` objects. Once again, you should be using your feature descriptor matching code from the last assignment.\n",
    "\n",
    "**FIGURE** I have included code under `Putting it all together` that, (1) computes features, (2) matches between them, (3) the homography to align the images, and (4) the plot showing the performance of the alignment. If you have finished implementing the previous functions, the final plot should show all the pieces working in harmony on the two transformed `sunflower` images I have provided! Run this code and include the resulting plot in your writeup, showing that you computed reasonable features/matches and the homography that aligns the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Matching Pipeline Helper + Plotting Code\n",
    "class Feature(object):\n",
    "    def __init__(self, x, y, radius):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.radius = radius\n",
    "        self.descriptor = None\n",
    "        \n",
    "def plot_circ_features(image, features, ax):\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    for f in features:\n",
    "        cir = plt.Circle((f.x, f.y), f.radius, color='r', fill=False)\n",
    "        ax.add_artist(cir)\n",
    "\n",
    "def get_gaussian_filter(n, sigma=1):\n",
    "    n = 2 * (n//2) + 1\n",
    "    h = (n + 1)//2\n",
    "    d = np.arange(h)\n",
    "    d = np.concatenate((d[::-1], d[1:]))\n",
    "    d = d[:, np.newaxis]\n",
    "    d_sq = d**2 + d.T ** 2\n",
    "    # Take the gaussian\n",
    "    g = np.exp(-d_sq/2/(sigma**2))\n",
    "    # Normalize\n",
    "    g = g/g.sum().sum()\n",
    "    return g\n",
    "\n",
    "def compute_local_orientation(image, loc_x, loc_y):\n",
    "    sobel_x = np.array([\n",
    "      [1, 0, -1],\n",
    "      [2, 0, -2],\n",
    "      [1, 0, -1]\n",
    "    ])\n",
    "\n",
    "    sobel_y = np.array([\n",
    "      [1, 2, 1],\n",
    "      [0, 0, 0],\n",
    "      [-1, -2, -1]\n",
    "    ])\n",
    "    \n",
    "    ir = loc_y\n",
    "    ic = loc_x\n",
    "    Ix = scipy.signal.convolve(\n",
    "        image, sobel_x, mode='same')\n",
    "    Iy = scipy.signal.convolve(\n",
    "        image, sobel_y, mode='same')\n",
    "    return np.arctan2(Iy[ir, ic], Ix[ir, ic])\n",
    "\n",
    "def compute_descriptor_for_feature(image, feature, half_pixel_width=5):\n",
    "    \"\"\"Gets descriptor patch for Feature object.\"\"\"\n",
    "    # Blur the image before computing orientation\n",
    "    # and downsampling\n",
    "    scale = feature.radius\n",
    "    filt = get_gaussian_filter(5*scale, scale/half_pixel_width)\n",
    "    scale_blurred_image = scipy.signal.convolve(\n",
    "        image, filt, mode='same')\n",
    "    \n",
    "    filt = get_gaussian_filter(5*scale, scale)\n",
    "    scale_blurred_image_more = scipy.signal.convolve(\n",
    "        image, filt, mode='same')\n",
    "\n",
    "        # Compute the orientation\n",
    "    orientation = compute_local_orientation(\n",
    "        scale_blurred_image_more, feature.x, feature.y)\n",
    "    \n",
    "    patch = get_scaled_rotated_patch(\n",
    "        image=scale_blurred_image,\n",
    "        feature_center_x=feature.x,\n",
    "        feature_center_y=feature.y,\n",
    "        feature_radius=feature.radius,\n",
    "        patch_radius=2*feature.radius,\n",
    "        feature_orientation=orientation,\n",
    "        half_pixel_width=5)\n",
    "    \n",
    "    # Center and normalize patch\n",
    "    return (patch - np.mean(patch))/np.std(patch)/np.prod(patch.shape)\n",
    "\n",
    "def compute_features_with_descriptors(image, sigmas, threshold):\n",
    "    features = compute_multi_scale_features(image, sigmas, threshold)\n",
    "    for feature in features:\n",
    "        feature.descriptor = compute_descriptor_for_feature(image, feature)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Matching: Code to implement for P3.3\n",
    "\n",
    "def compute_multi_scale_features(image, sigmas, threshold):\n",
    "    \"\"\"Should return a list of 'Feature' objects.\"\"\"\n",
    "    raise NotImplementedError(\"Your solution from the last assignment.\")\n",
    "\n",
    "def compute_feature_matches(fsa, fsb):\n",
    "    \"\"\"Computes matches between two lists of Feature objects.\n",
    "    Returns a list of matched feature pairs, [fa, fb]\"\"\"\n",
    "    raise NotImplementedError(\"Your code from the last assignmnent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the images\n",
    "imagea = load_image('sunflowers_transformed_a.png')[:, :, 0]\n",
    "imageb = load_image('sunflowers_transformed_b.png')[:, :, 0]\n",
    "\n",
    "# Compute the features (with descriptors)\n",
    "fsa = compute_features_with_descriptors(imagea, sigmas, 0.8)\n",
    "fsb = compute_features_with_descriptors(imageb, sigmas, 0.8)\n",
    "\n",
    "# Compute the matches\n",
    "matches = compute_feature_matches(fsa, fsb)\n",
    "\n",
    "# Convert to our other style of \"matches\"\n",
    "# so that we can use the `solve_homography_ransac`\n",
    "# function.\n",
    "np_matches = np.array([\n",
    "    [fa.x, fa.y, fb.x, fb.y]\n",
    "    for fa, fb in matches])\n",
    "\n",
    "# Solve for the homography matrix\n",
    "H = solve_homography_ransac(np_matches)\n",
    "print(f\"Computed Homography: \\n{H}\")\n",
    "print(f\"Total Compute Time: {time.time() - start_time}\")\n",
    "\n",
    "# Visualize the results\n",
    "visualize_computed_transform(\n",
    "    imagea, imageb, H, np_matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Feature Matching with OpenCV\n",
    "\n",
    "**Please do not attempt this question until the previous questions are completed; I would like you to try to get your system mostly working before trying a \"professional\" package.**\n",
    "\n",
    "Follow the [OpenCV tutorial](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html) to implement feature matching (copy-pasting code is expected here).\n",
    "\n",
    "Next, you will do this on an object of your own!\n",
    "\n",
    "**TASK** Find a *\n",
    "\n",
    "**TASK** \n",
    "\n",
    "**FIGURE & DISCUSSION** Generate an image like the one in the tutorial but for the two transformed sunflower images I have included. How does the performance (e.g., in terms of the number of features or accuracy of matches) of the OpenCV system compare to the system you implemented? How much faster (roughly, I do not need a precise number) is the OpenCV system compared to yours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS682)",
   "language": "python",
   "name": "cs682venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
